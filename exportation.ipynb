{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark-dist-explore in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (0.1.8)\n",
      "Requirement already satisfied: pandas in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from pyspark-dist-explore) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from pyspark-dist-explore) (3.5.1)\n",
      "Requirement already satisfied: scipy in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from pyspark-dist-explore) (1.8.0)\n",
      "Requirement already satisfied: numpy in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from pyspark-dist-explore) (1.21.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from matplotlib->pyspark-dist-explore) (9.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from matplotlib->pyspark-dist-explore) (4.31.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from matplotlib->pyspark-dist-explore) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from matplotlib->pyspark-dist-explore) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from matplotlib->pyspark-dist-explore) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from matplotlib->pyspark-dist-explore) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from matplotlib->pyspark-dist-explore) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from pandas->pyspark-dist-explore) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/sidiras/PycharmProjects/NoSql_Data_Model/venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->pyspark-dist-explore) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark-dist-explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- stdorToU: string (nullable = true)\n",
      " |-- DateTime: string (nullable = true)\n",
      " |-- KWH/hh (per half hour) : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "df = spark.read.option(\"header\",True).csv(\"data/CC_LCL-FullData.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167932474"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"DateTime\",df.DateTime.astype('Timestamp'))\n",
    "#https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/\n",
    "df3=df.select(col(\"DateTime\"),\n",
    "             col(\"LCLid\").alias((\"id\")),\n",
    "            col(\"stdorToU\").alias(\"std\"),\n",
    "            col(\"KWH/hh (per half hour) \").alias(\"kwh\"),\n",
    "     year(col(\"DateTime\")).alias(\"year\"),\n",
    "     month(col(\"DateTime\")).alias(\"month\"),\n",
    "    dayofmonth(col(\"DateTime\")).alias(\"date\"),\n",
    "    hour(col(\"DateTime\")).alias(\"hour\"),\n",
    "    minute(col(\"DateTime\")).alias(\"min\"),\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167932474"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The dataset  consist from 165M++ rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Weather data\n",
    "df_weather = pd.read_csv(\"data/add/weather_hourly_darksky.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Household info data\n",
    "df_household = pd.read_csv(\"data/add/informations_households.csv\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24158"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "#filter the data for one househd\n",
    "df3.filter(df3.id=='MAC000002').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#household convert spark  dataframe\n",
    "householdd=spark.createDataFrame(df_household)\n",
    "\n",
    "#Join two DataSet\n",
    "df4=df3.join(householdd, df3.id == householdd.LCLid, 'left')\n",
    "df5=df4.withColumn(\"kwh\", df4.kwh.cast('double'))\n",
    "\n",
    "\n",
    "#convert string energy cosumtion data to double\n",
    "df5=df4.withColumn(\"kwh\", df4.kwh.cast('double'))\n",
    "\n",
    "#filter thepip install pyspark-dist-explore data to perform to  a smaller sample all the\n",
    "dff=df3.filter(df3.id=='MAC000002')\n",
    "\n",
    "\n",
    "#innerjoin with household and energy\n",
    "#Apply filter to have better result\n",
    "df6=dff.join(householdd, dff.id == householdd.LCLid, 'left')\n",
    "#convert string to Chriss\n",
    "df7=df6.withColumn(\"kwh\", df6.kwh.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before casting\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- std: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- sum(kwh): double (nullable = true)\n",
      "\n",
      "After casting\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- std: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- sum(kwh): double (nullable = true)\n",
      " |-- hours: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df8=df7.groupby(\"id\",\"std\",\"year\",\"month\",\"date\",\"hour\").sum(\"kwh\")\n",
    "\n",
    "\n",
    "print(\"Before casting\")\n",
    "df8.printSchema()\n",
    "df8_hours=df8.withColumn(\"hours\",df8['hour'].cast('integer'))\n",
    "print(\"After casting\")\n",
    "df8_hours.printSchema()\n",
    "\n",
    "pivot_df = df8_hours.groupby(\"id\",\"year\",\"month\",\"date\").pivot(\"hours\").sum(\"sum(kwh)\")\n",
    "\n",
    "\n",
    "#apply for all data in the dataset the aggregation sums\n",
    "\n",
    "#first we aggregate in hours , in this data set we have every half hour so we sum every hour\n",
    "# In the Eveddent dataset we do not need to aggredate every hour\n",
    "df_all8=df5.groupby(\"id\",\"std\",\"year\",\"month\",\"date\",\"hour\").sum(\"kwh\")\n",
    "#make ne aggregation per data\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "#create agragate columns for one day avg/max.min/standard division and sum of energy \n",
    "df_all_statistics=df5.groupby(\"id\",\"std\",\"year\",\"month\",\"date\").agg(f.sum(\"kwh\"),f.avg(\"kwh\"),f.max(\"kwh\"),f.min(\"kwh\"),f.count(\"kwh\"),f.stddev_pop(\"kwh\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the df\n",
    "df_all_statistics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='MAC000032', std='Std', year=2011, month=12, date=7, sum(kwh)=3.5269999999999997, avg(kwh)=0.15334782608695652, max(kwh)=0.692, min(kwh)=0.015, count(kwh)=23, stddev_pop(kwh)=0.20240609388825276)\n",
      "Row(id='MAC000032', std='Std', year=2011, month=12, date=8, sum(kwh)=17.6700001, avg(kwh)=0.3681250020833333, max(kwh)=2.5050001, min(kwh)=0.01, count(kwh)=48, stddev_pop(kwh)=0.5694364846124748)\n",
      "Row(id='MAC000032', std='Std', year=2011, month=12, date=9, sum(kwh)=18.41300040000001, avg(kwh)=0.38360417500000016, max(kwh)=2.5680001, min(kwh)=0.01, count(kwh)=48, stddev_pop(kwh)=0.6339574196381306)\n",
      "Row(id='MAC000032', std='Std', year=2011, month=12, date=10, sum(kwh)=21.75300010000001, avg(kwh)=0.4531875020833335, max(kwh)=2.586, min(kwh)=0.01, count(kwh)=48, stddev_pop(kwh)=0.6246162401870999)\n",
      "Row(id='MAC000032', std='Std', year=2011, month=12, date=11, sum(kwh)=19.146000000000004, avg(kwh)=0.3988750000000001, max(kwh)=2.648, min(kwh)=0.01, count(kwh)=48, stddev_pop(kwh)=0.6859822830863299)\n"
     ]
    }
   ],
   "source": [
    "#The previous print for better visual each row\n",
    "for i in df_all_statistics.head(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df_all_statistics is the final dayly aggregation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3510433"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_all_statistics is the final dayly aggregation\n",
    "df_all_statistics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before casting\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_all8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-3cbb737e34c7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Before casting\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf_all8\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mdf_all_hours\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdf_all8\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"hours\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf_all8\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'hour'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'integer'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"After casting\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_all8' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Before casting\")\n",
    "df_all8.printSchema()\n",
    "\n",
    "df_all_hours=df_all8.withColumn(\"hours\",df_all8['hour'].cast('integer'))\n",
    "print(\"After casting\")\n",
    "df_all_hours.printSchema()\n",
    "pivot_df_all=df_all_hours.groupby(\"id\",\"year\",\"month\",\"date\").pivot(\"hours\").sum(\"sum(kwh)\")\n",
    "#dataset = pivot_df_all.groupby(\"id\",\"year\",\"month\",\"date\").sum(\"sum(kwh)\").avg(\"sum(kwh)\")(\"sum(kwh)\").min(\"sum(kwh)\").count(\"sum(kwh)\").std(\"sum(kwh)\")\n",
    "\n",
    "pivot_df_all.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reading Household info data\n",
    "df_household = pd.read_csv(\"data/add/informations_households.csv\", encoding=\"utf-8\")\n",
    "\n",
    "#household convert spark  dataframe\n",
    "householdd=spark.createDataFrame(df_household)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_4=df3.withColumn(\"kwh\", df3.kwh.cast('double'))\n",
    "#Join two DataSet\n",
    "df4=df3.join(householdd, df3.id == householdd.LCLid, 'left')\n",
    "\n",
    "df5=df4.withColumn(\"kwh\", df4.kwh.cast('double'))\n",
    "\n",
    "dff_2012=df5.filter(df5.year=='2012')\n",
    "dff_2013=df5.filter(df5.year=='2013')\n",
    "dff_2014=df5.filter(df5.year=='2014')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df8_hours=dff_2012.withColumn(\"hours\",dff_2012['hour'].cast('integer'))\n",
    "\n",
    "\n",
    "Energy_Total2012 = dff_2012.groupby(\"year\").sum(\"kwh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#aggragate function and hours cosumtion at coloumns\n",
    "pivot_df_2012 = df_4.groupby(\"id\",\"year\",\"month\",\"date\").pivot(\"hour\").sum(\"kwh\")\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Energy_Months2012 = dff_2012.groupby(\"month\").sum(\"kwh\").orderBy(\"month\").toPandas()\n",
    "\n",
    "Energy_Months2012.plot(kind='barh',x='month',y='sum(kwh)',colormap='winter_r')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python385jvsc74a57bd0ed4335d6cbe1b4beeab2279141ac2539677c122e695ba2529d61d5b04ad0c51b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}